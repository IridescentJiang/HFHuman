# HFHuman: High-Fidelity Human Reconstruction from Single Image with Multi-Modality Fusion
Official PyTorch implementation for the paper:

> **[HFHuman: High-Fidelity Human Reconstruction from Single Image with Multi-Modality Fusion](https://ieeexplore.ieee.org/abstract/document/11207513/) [TVCG 2025]**
>
> Yiming Jiang, Wenfeng Song\*,  Shuai Li\*, Aimin Hao
>
> \* Wenfeng Song and Shuai Li are corresponding authors. Email: songwenfenga@gmail.com, lishuai@buaa.edu.cn



![image-20251027171531822](https://pic-jym.oss-cn-beijing.aliyuncs.com/img/image-20251027171531822.png)

Given a single RGB image, **HFHuman** can reconstruct a detailed 3D human body model by extracting and fusing the corresponding normal map, depthmap,  and  human  body  parametric  geometry  mode.



## The Pipeline of Our Method

![image-20251027171540632](https://pic-jym.oss-cn-beijing.aliyuncs.com/img/image-20251027171540632.png)

Given a single RGB image, we use the Prior Knowledge-Based Estimation models to obtain the corresponding geometry, depth,  and  normal  map.  During  the  training  phase,  the  human  body  model  parameters  are  rectified  using  the  Progressive  Geometry  Rectification  process. Lastly,  we  leverage  the  Multi-Modality  Adaptor  to  fuse  the  pixel  features  provided  by  the  depth  map  with  the  voxel  features  provided  by  the  human  body parametric model, enabling us to reconstruct a detailed geometric human body model.



## Environmental Preparation

- Ubuntu (Linux)
- Python 3.6+
- Pytorch 1.10.1
- CUDA 11.1
- Pytorch3D
- Trimesh

Create environment:

```bash
conda env create -f environment.yml
```

Other necessary packages:
```bash
pip install -r requirements.txt
```



## Training
Before training the main network, first train the Normal Estimator and Depth Refiner. 

Then use the results generated by the trained Normal Estimator and Depth Refiner as input to train the main network.

#### 1) Normal Estimator Training

* Train Normal Estimator:

```bash
python train_normalmodel.py
```
* Generate normal map by pre-trained Normal Estimator:

```bash
python generatemaps_normalmodel.py
```

#### 2) Depth Refiner Training

* Train Depth Refiner:

```bash
python train_depthrefiner.py
```
* Generate normal map by pre-trained Depth Refiner:

```bash
python generatemaps_depthrefiner.py
```

#### 3) Main Network Training

```bash
python train_HFHuman.py
```
Configuration can be set in lib/options.py file. 



## Prerequisites:
#### 1) Request permission to use THuman2.0 dataset (https://github.com/ytrock/THuman2.0-Dataset). 
After permission granted, download the dataset (THuman2.0_Release). Put the "THuman2.0_Release" folder inside the "rendering_script" folder. 

#### 2) Rendering Images, Normal maps, and Depth maps.
Run render_full_mesh.py, render_normal_map_of_full_mesh.py, and render_depth_map_of_full_mesh.py to generate RGB image, normal map, and depth map respectively. For example, to render the RGB image of subject '0501' at a yaw angle of '90' degrees, run: 
`blender blank.blend -b -P render_full_mesh.py -- 0501 90`
Replace render_full_mesh.py with render_normal_map_of_full_mesh.py or render_depth_map_of_full_mesh.py to render normal map and depth map respectively. 

#### 3) Obtaining SMPL Models.
We use HybrIK (https://github.com/Jeff-sjtu/HybrIK) as our Geometry Estimator. 

We use the SMPL model predicted by HybrIK as input, and the SMPL model provided by the THuman2.0 dataset as ground truth for supervision.

#### 4) Obtaining Depth Maps for Depth Refiner.
We use DepthAnythingV2 (https://github.com/DepthAnything/Depth-Anything-V2) as the Depth Estimator to get depth maps, which are then used as part of the input to train the Depth Refiner.

#### 5) Sampling Points.
Use sample_points.py to sample points from the dataset Mesh to speed up training.



## Citation

```bibtex
@ARTICLE{11207513,
  author={Jiang, Yiming and Song, Wenfeng and Li, Shuai and Hao, Aimin},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={HFHuman: High-Fidelity Human Reconstruction from Single Image with Multi-Modality Fusion}, 
  year={2025},
  pages={1-12},
  doi={10.1109/TVCG.2025.3623198}}
```



## **Acknowledgement**

We heavily borrow the code from [IntegratedPIFu](https://github.com/kcyt/IntegratedPIFu) and [PaMIR](https://github.com/ZhengZerong/PaMIR).  We also gratefully acknowledge the THU for providing the [THuman2.0 dataset](https://github.com/ytrock/THuman2.0-Dataset) dataset. Any third-party packages are owned by their respective authors and must be used under their respective licenses.
