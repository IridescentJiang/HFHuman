# HFHuman
Official implementation code of "HFHuman: High-Fidelity Human Reconstruction from Single Image with Multi-Modality Fusion"

## Environmental Preparation
- Ubuntu (Linux)
- Python 3.6+
- Pytorch 1.10.1
- CUDA 11.1
- Pytorch3D
- Trimesh

Create environment:

```bash
conda env create -f environment.yml
```

Other necessary packages:
```bash
pip install -r requirements.txt
```



## Training
Before training the main network, first train the Normal Estimator and Depth Refiner. 

Then use the results generated by the trained Normal Estimator and Depth Refiner as input to train the main network.

### 1) Normal Estimator Training

Train Normal Estimator:
```bash
python train_normalmodel.py
```
Generate normal map by pre-trained Normal Estimator:
```bash
python generatemaps_normalmodel.py
```

### 2) Depth Refiner Training
Train Depth Refiner:
```bash
python train_depthrefiner.py
```
Generate normal map by pre-trained Depth Refiner:
```bash
python generatemaps_depthrefiner.py
```

### 3) Main Network Training
```bash
python train_HFHuman.py
```
Configuration can be set in lib/options.py file. 



## Prerequisites:
### 1) Request permission to use THuman2.0 dataset (https://github.com/ytrock/THuman2.0-Dataset). 
After permission granted, download the dataset (THuman2.0_Release). Put the "THuman2.0_Release" folder inside the "rendering_script" folder. 

### 2) Rendering Images, Normal maps, and Depth maps.
Run render_full_mesh.py, render_normal_map_of_full_mesh.py, and render_depth_map_of_full_mesh.py to generate RGB image, normal map, and depth map respectively. For example, to render the RGB image of subject '0501' at a yaw angle of '90' degrees, run: 
`blender blank.blend -b -P render_full_mesh.py -- 0501 90`
Replace render_full_mesh.py with render_normal_map_of_full_mesh.py or render_depth_map_of_full_mesh.py to render normal map and depth map respectively. 

### 3) Obtaining SMPL Models.
We use HybrIK (https://github.com/Jeff-sjtu/HybrIK) as our Geometry Estimator. 

We use the SMPL model predicted by HybrIK as input, and the SMPL model provided by the THuman2.0 dataset as ground truth for supervision.

### 4) Obtaining Depth Maps for Depth Refiner.
We use DepthAnythingV2 (https://github.com/DepthAnything/Depth-Anything-V2) as the Depth Estimator to get depth maps, which are then used as part of the input to train the Depth Refiner.

### 5) Sampling Points.
Use sample_points.py to sample points from the dataset Mesh to speed up training.



